# KNN-From-Scratch-With-visualizations
"ML models implemented from scratch using NumPy and Pandas only"

ðŸ©º KNN Classifier from Scratch â€“ Diabetes Prediction

ðŸ“Œ Introduction

This project implements the K-Nearest Neighbors (KNN) algorithm from scratch (without sklearn classifiers) to predict diabetes using the PIMA Indians Diabetes Dataset.

KNN is a lazy learner: it predicts by finding the closest neighbors in the training set using Euclidean distance and applying majority voting.


---

ðŸ“Š Dataset

Source: PIMA Indians Diabetes Dataset

Features: Glucose, Blood Pressure, BMI, Age, etc.

Target (Outcome):

0 â†’ Non-Diabetic

1 â†’ Diabetic




---

ðŸ§® Key Formulas

Distance (Euclidean)


$$d(x, y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}$$

Accuracy


$$\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}$$

Precision


$$\text{Precision} = \frac{TP}{TP + FP}$$

Recall


$$\text{Recall} = \frac{TP}{TP + FN}$$

F1-score


$$F1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$$


---

ðŸ“ˆ Results & Visualizations

Confusion Matrix plotted for model evaluation.

Accuracy, Precision, Recall, F1 compared for different values of k.

Best k selected based on F1-score for Class 1 (Diabetic).



---

ðŸš€ How to Run

pip install numpy pandas matplotlib seaborn scikit-learn
python knn_from_scratch.py


---

âœ… Conclusion

KNN works well but is sensitive to class imbalance and choice of k.

Class 1 (Diabetic) is harder to classify due to fewer samples and overlapping features.

Best k found using F1-score optimization.



---
